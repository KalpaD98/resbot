{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_words():\n",
    "    return {\n",
    "    'feature 1' :  ['service', 'experience', 'look', 'flavor', 'star', 'dish', 'store', 'tasty', 'come', 'return'],\n",
    "    'feature 2' :  ['order', 'lunch' ,'dinner', 'enjoy', 'friend', 'breakfast', 'onion', 'soup','point', 'town'],\n",
    "    'feature 3' :  ['eat', 'way', 'option', 'feel', 'portion', 'bowl', 'start', 'house', 'size','think'],\n",
    "    'feature 4' :  ['visit', 'staff', 'night', 'serve', 'work', 'chip', 'change', 'let', 'counter','taste'],\n",
    "    'feature 5' :  ['chicken', 'fry', 'sandwich', 'roll', 'bread', 'tender', 'make', 'shrimp','appetizer', 'grill'],\n",
    "    'feature 6' :  ['time', 'year', 'spot', 'bit', 'couple', 'use', 'water', 'thank', 'treat', 'dog'],\n",
    "    'feature 7' :  ['location', 'item', 'space', 'quality', 'share', 'stuff', 'plan', 'menu','favorite', 'trip'],\n",
    "    'feature 8' :  ['place', 'day', 'wait', 'minute', 'choice', 'delivery', 'crispy', 'pay', 'tea','bite'],\n",
    "    'feature 9' :  ['pizza', 'review', 'beer', 'parking', 'style', 'park', 'stop', 'street', 'plenty','walk'],\n",
    "    'feature 10' :  ['restaurant', 'home', 'rice', 'ask', 'pork', 'need', 'dine', 'customer', 'pack','wine'],\n",
    "    'feature 11' :  ['food', 'thing', 'lot', 'plate', 'cup', 'truck', 'taste', 'chain', 'juice','protein'],\n",
    "    'feature 12' :  ['menu', 'meal', 'people', 'price', 'salad', 'potato', 'business', 'head','ingredient', 'reason'],\n",
    "    'feature 13' :  ['seat', 'hour', 'pick', 'piece', 'half', 'person', 'course', 'grab', 'deal','note'],\n",
    "    'feature 14' :  ['bar', 'table', 'drink', 'sit', 'cook', 'fun', 'tell', 'steak', 'honey', 'cold'],\n",
    "    'feature 15' :  ['try', 'love', 'area', 'room', 'dessert', 'kind', 'reservation', 'locate', 'miss','manager'],\n",
    "    'feature 16' :  ['cheese', 'end', 'slice', 'husband', 'bring', 'pie', 'crust', 'sweet', 'base','group'],\n",
    "    'feature 17' :  ['coffee', 'egg', 'bacon', 'tomato', 'cake', 'mushroom', 'pepper', 'sausage','pickle', 'mix'],\n",
    "    'feature 18' :  ['sauce', 'drive', 'family', 'door', 'today', 'touch', 'heat', 'distance', 'boy','face'],\n",
    "    'feature 19' :  ['meat', 'line', 'beef', 'bun', 'wall', 'morning', 'establishment', 'phone', 'pub','stand'],\n",
    "    'feature 20' :  ['spicy', 'hand', 'mask', 'super', 'car', 'window', 'purchase', 'toast', 'kitchen','sign'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_features(text, feature_words):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_token(token):\n",
    "        token = token.lower()\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        return token\n",
    "\n",
    "    def preprocess_sentence(sentence):\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [preprocess_token(token) for token in tokens if token.isalpha()]\n",
    "        tokens = [token for token in tokens if token not in stopwords_set]\n",
    "        return tokens\n",
    "\n",
    "    preprocessed_text = [preprocess_sentence(sentence) for sentence in sent_tokenize(text)]\n",
    "\n",
    "    relevant_sentences = {}\n",
    "    for feature_word, related_words in feature_words.items():\n",
    "        relevant_sentences[feature_word] = []\n",
    "        for sentence_tokens in preprocessed_text:\n",
    "            if any((feature_word in sentence_tokens) or (related_word in sentence_tokens) for related_word in related_words):\n",
    "                relevant_sentences[feature_word].append(' '.join(sentence_tokens))\n",
    "\n",
    "    return relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_df(df, feature_words):\n",
    "    df['feature_summaries'] = df['review'].apply(lambda x: extract_sentences_with_features(x, feature_words))\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        relevant_sentences = row['feature_summaries']\n",
    "        dic = {}\n",
    "        for feature_word, sentences in relevant_sentences.items():\n",
    "            joined_sentence = ' '.join(sentences)\n",
    "            dic[feature_word] = joined_sentence\n",
    "        df.at[index, 'feature_summaries'] = dic\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_score(sentence):\n",
    "    blob = TextBlob(sentence)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_dfs(df_sent):\n",
    "    df_user_res_grp = df_sent.groupby([\"business_id\", \"user_id\"]).apply(lambda x: x[x != 0].mean(numeric_only=True))\n",
    "    df_user_res_grp.reset_index(inplace=True)\n",
    "    df_user_res_grp.fillna(0.0, inplace=True)\n",
    "\n",
    "    df_sent_user = df_sent.drop(columns=[\"business_id\"])\n",
    "    df_user_grp = df_sent_user.groupby([\"user_id\"]).apply(lambda x: x.abs()[x != 0].mean(numeric_only=True))\n",
    "    df_user_grp.reset_index(inplace=True)\n",
    "    df_user_grp.fillna(0.0, inplace=True)\n",
    "\n",
    "    df_sent_res = df_sent.drop(columns=[\"user_id\"])\n",
    "    df_res_grp = df_sent_res.groupby([\"business_id\"]).apply(lambda x: x[x != 0].mean(numeric_only=True))\n",
    "    df_res_grp.reset_index(inplace=True)\n",
    "    df_res_grp.fillna(0.0, inplace=True)\n",
    "\n",
    "    return df_user_res_grp, df_user_grp, df_res_grp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering(target_user, df_user_res_grp, df_user_grp, features, similarity_threshold=0.7):\n",
    "    selected_user_df = df_user_grp[df_user_grp['user_id'] == target_user]\n",
    "    sel_usr_zro_cols = selected_user_df.columns[selected_user_df.eq(0).any()].tolist()\n",
    "    sel_usr_features = selected_user_df[features].columns[selected_user_df[features].any()]\n",
    "\n",
    "    selected_user_features = df_user_grp.loc[df_user_grp['user_id'] == target_user, sel_usr_features].values\n",
    "\n",
    "    user_features = df_user_grp[sel_usr_features].values\n",
    "\n",
    "    user_similarity_matrix = cosine_similarity(selected_user_features, user_features)\n",
    "\n",
    "    similarity_df = pd.DataFrame({\n",
    "        'user_id': df_user_grp['user_id'],\n",
    "        'similarity_score': user_similarity_matrix.flatten()\n",
    "    })\n",
    "    similarity_df = similarity_df.sort_values('similarity_score', ascending=False)\n",
    "\n",
    "    similarity_df = similarity_df.drop(similarity_df[similarity_df['user_id'] == target_user].index)\n",
    "\n",
    "    similar_users = similarity_df[similarity_df['similarity_score'] > similarity_threshold]\n",
    "\n",
    "    filtered_user_ids = similar_users['user_id'].tolist()\n",
    "\n",
    "    df_user_res_wt_sim = pd.merge(df_user_res_grp, similar_users, on='user_id')\n",
    "    df_user_res_wt_sim = df_user_res_wt_sim.drop(columns=sel_usr_zro_cols)\n",
    "    df_user_res_wt_sim[sel_usr_features] = df_user_res_wt_sim[sel_usr_features].mul(df_user_res_wt_sim['similarity_score'], axis=0)\n",
    "\n",
    "    aggregated_ratings = df_user_res_wt_sim[df_user_res_wt_sim['user_id'].isin(filtered_user_ids)].groupby('business_id')[sel_usr_features].apply(lambda x: x[x != 0].mean())\n",
    "    aggregated_ratings.fillna(0.0, inplace=True)\n",
    "\n",
    "    predicted_ratings_scr = aggregated_ratings.apply(lambda x: x[x != 0].mean(), axis=1)\n",
    "    predicted_ratings_scr = predicted_ratings_scr.dropna()\n",
    "\n",
    "    normalized_ratings = (predicted_ratings_scr - predicted_ratings_scr.min()) / (predicted_ratings_scr.max() - predicted_ratings_scr.min()) * 4 + 1\n",
    "\n",
    "    all_businesses = normalized_ratings.sort_values(ascending=False)\n",
    "\n",
    "    df_cf = pd.DataFrame({'business_id': all_businesses.index, 'cf_predicted_rating': all_businesses.values})\n",
    "\n",
    "    return df_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_filtering(df_user_grp, df_res_grp, target_user):\n",
    "    selected_user_preferences = df_user_grp.loc[df_user_grp['user_id'] == target_user].drop('user_id', axis=1)\n",
    "\n",
    "    similarity_scores = cosine_similarity(selected_user_preferences, df_res_grp.drop('business_id', axis=1))\n",
    "\n",
    "    df_cbf = pd.DataFrame({\n",
    "        'business_id': df_res_grp['business_id'],\n",
    "        'similarity_score': similarity_scores.flatten()\n",
    "    })\n",
    "\n",
    "    min_score = df_cbf['similarity_score'].min()\n",
    "    max_score = df_cbf['similarity_score'].max()\n",
    "\n",
    "    df_cbf['cbf_predicted_rating'] = 1 + (df_cbf['similarity_score'] - min_score) * (5 - 1) / (max_score - min_score)\n",
    "    df_cbf = df_cbf.drop('similarity_score', axis=1)\n",
    "\n",
    "    df_cbf = df_cbf.sort_values('cbf_predicted_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df_cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(colab_df, content_df, content_weight, collab_filtering_weight, n_recommendations):\n",
    "    colab_df = colab_df.drop(columns=['customer_stars'])\n",
    "    hy_df = pd.merge(content_df, colab_df, on='business_id')\n",
    "\n",
    "    hy_df['weighted_hybrid_score'] = hy_df['cbf_predicted_rating'] * content_weight + hy_df['cf_predicted_rating'] * collab_filtering_weight\n",
    "\n",
    "    ranked_business_ids = hy_df.sort_values(by='weighted_hybrid_score', ascending=False)['business_id'].head(n_recommendations).tolist()\n",
    "    return ranked_business_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation(user_id, df, n_restaurants=10):\n",
    "    df = df.drop(columns=['categories','address', 'state_', 'city', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open', 'hours', 'review_id', 'useful', 'funny', 'cool', 'date_', 'name' ])\n",
    "    df = df.rename(columns={'text_': 'review'})\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    feature_words = get_feature_words()\n",
    "    \n",
    "    df_sent = get_feature_df(df, feature_words)\n",
    "    feature_name_list = list(eval(df_sent['feature_summaries'][0]).keys())\n",
    "\n",
    "    df_sent[\"sentiment_scores\"] = df_sent[\"feature_summaries\"].apply(lambda row: {feature: calculate_sentiment_score(sentence) for feature, sentence in eval(row).items()})\n",
    "    sentiment_scores_df = pd.DataFrame(df_sent[\"sentiment_scores\"].tolist())\n",
    "    df_sent = pd.concat([df_sent, sentiment_scores_df], axis=1)\n",
    "    df_sent = df_sent.drop(columns=[ 'review', 'feature_summaries', 'sentiment_scores'])\n",
    "\n",
    "    df_user_res_feature_scr, df_user_feature_scr, df_res_feature_scr = get_rec_dfs(df_sent)\n",
    "\n",
    "    cf_df = collaborative_filtering(target_user=user_id, df_user_res_grp=df_user_res_feature_scr, df_user_grp=df_user_feature_scr, features=feature_name_list, similarity_threshold=0.7)\n",
    "    cbf_df = content_based_filtering(df_user_grp=df_user_feature_scr, df_res_grp=df_res_feature_scr, target_user=user_id)\n",
    "    \n",
    "    ranked_rest_ids = hybrid_recommendation(colab_df=cf_df, content_df=cbf_df, content_weight=0.5, collab_filtering_weight=0.5, n_recommendations=n_restaurants)\n",
    "    \n",
    "    return ranked_rest_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = '/content/drive/MyDrive/FYP/data/postcovid_reviews.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "result = recommendation('0', df, 10)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
